// uses Nq x Nq threads

// (grad v, grad q) + ([v], n.{grad q}) + (n.{grad v}, [q]) + ([v],tau*[q])
// (grad v, grad q) + (-v, n.{grad q}) + (n.grad v-, (q+ - q-)) + (-v,tau*(q+-q-))
// (grad v, grad q + delta*n*(q+-q-)) + (-v, n.{grad q} + tau*(q+-q-))
// (G*grad^ v, G*grad^ q + delta*n*(q+-q-)) + (-v, n.{grad q} + tau*(q+-q-))
// (grad^ v, G'*G*grad^ q + delta*(G'*n)*(q+ - q-)) + (-v, n.{grad q} + tau*(q+-q-))

// independent faces: (0,5), (1,3), (2,4)

kernel void ScalarEllipticSlicedOperator3D(const int K,
					   const dfloat * restrict D,
					   const dfloat * restrict vgeo,
					   const dfloat * restrict sgeo,
					   const dfloat * restrict q,
 					   const dfloat * restrict qP,
					   const dfloat * restrict dqdnM,
					   const dfloat * restrict dqdnP,
					   dfloat * restrict Aq){
  

  // can split into outer0/inner1
  for(int e=0;e<K;++e;outer0){
    
    // 4*p_Nq^2 shared dfloats (N=4 => 400 bytes)
    shared dfloat  s_D[p_Nq][p_Nq];
    shared dfloat  s_q[p_Nq][p_Nq];
    shared dfloat s_Dq0[p_Nq][p_Nq];
    shared dfloat s_Dq1[p_Nq][p_Nq];

    // 2*p_Nq + 3 exclusive floats (N=4 => 14 registers blocked)
    exclusive dfloat r_q[p_Nq];
    exclusive dfloat r_Aq[p_Nq];
    exclusive dfloat r_dq05, r_dq13, r_dq24; // save jump in q

    // prefetch to registers and shared memory 
    for(int j=0;j<p_Nq;++j;inner1){
      for(int i=0;i<p_Nq;++i;inner0){
	for(int k=0;k<p_Nq;++k){ 
	  const int id = i + j*p_Nq + k*p_Nq2 + e*p_Nq3;
	  r_q[k] = q[id];
	  r_Aq[k] = 0.f;
	}
	s_D[j][i] = D[j*p_Nq+i];
      }
    }    
    
    // compute first derivatives
    for(int k=0;k<p_Nq;++k){
      
      // do not need a barrier here - since there is a barrier either side of s_q access

      // copy to shared
      for(int j=0;j<p_Nq;++j;inner1){
	for(int i=0;i<p_Nq;++i;inner0){
	  s_q[j][i] = r_q[k];
	}
      }
      
      barrier(localMemFence);
      
      for(int j=0;j<p_Nq;++j;inner1){
	for(int i=0;i<p_Nq;++i;inner0){
	  
	  const int vidG = e*p_Nvgeo*p_Nq3 + k*p_Nq2 + j*p_Nq1 + i;
	  
	  const dfloat G00 = vgeo[vidG]; vidG += p_Nq3;
	  const dfloat G01 = vgeo[vidG]; vidG += p_Nq3;
	  const dfloat G02 = vgeo[vidG]; vidG += p_Nq3;
	  const dfloat G11 = vgeo[vidG]; vidG += p_Nq3;
	  const dfloat G12 = vgeo[vidG]; vidG += p_Nq3;
	  const dfloat G22 = vgeo[vidG]; 
	  
	  dfloat qr = 0.f, qs = 0.f, qt = 0.f;
	  
	  for(int n=0;n<p_Nq;++n){
	    qr += s_D[i][n]*s_q[j][n];
	    qs += s_D[j][n]*s_q[n][i];
	    qt += s_D[k][n]*r_q[n];
	  }
	  
	  dfloat r_Dq0 = G00*qr + G01*qs + G02*qt;
	  dfloat r_Dq1 = G01*qr + G11*qs + G12*qt;
	  dfloat r_Dq2 = G02*qr + G12*qs + G22*qt;
    
	  // add terms at face nodes to contribute to (grad v, [q]) terms
	  // faces: (0:5)
	  if(k==0 || k==Nq-1){
	    const int f = (k==0)?0:5;
	    const int idG = p_Nsgeo*e*p_Nq2*p_Nfaces + p_Nsgeo*f*p_Nq2 + j*p_Nq + i;
	    const int idM = e*p_Nq2*p_Nfaces + f*p_Nq2 + j*p_Nq + i;

	    // compute trace jump
	    r_dq05 = qP[idM]-r_q[k];
	    
	    // assume sgeo contains G'*n
	    r_Dq0 += sgeo[idG]*r_dq05; idG+=p_Nq2;
	    r_Dq1 += sgeo[idG]*r_dq05; idG+=p_Nq2;
	    r_Dq2 += sgeo[idG]*r_dq05; 
	  }
	  
	  // faces: (1,3)
	  if(j==0 || j==p_Nq-1){ 
	    const int f = (j==0) ? 1:3;
	    const int idG = p_Nsgeo*e*p_Nq2*p_Nfaces + p_Nsgeo*f*p_Nq2 + k*p_Nq + i;
	    const int idM = e*p_Nq2*p_Nfaces + f*p_Nq2 + k*p_Nq + i;
	    
	    // compute trace jump
	    r_dq13 = qP[idM]-r_q[k];
	    
	    // assume sgeo contains G'*n
	    r_Dq0 += sgeo[idG]*r_dq13; idG+=p_Nq2;
	    r_Dq1 += sgeo[idG]*r_dq13; idG+=p_Nq2;
	    r_Dq2 += sgeo[idG]*r_dq13;
	  }

	  // faces: (2,4)
	  if(i==0 || i==p_Nq-1){ 
	    const int f = (i==0) ? 4:2;
	    const int idG = p_Nsgeo*e*p_Nq2*p_Nfaces + p_Nsgeo*f*p_Nq2 + k*p_Nq + j; 
	    const int idM = e*p_Nq2*p_Nfaces + f*p_Nq2 + k*p_Nq + j; 
	    
	    // compute trace jump
	    r_dq24 = qP[idM]-r_q[k];
	    
	    // assume sgeo contains G'*n
	    r_Dq0 += sgeo[idG]*r_dq24; idG+=p_Nq2;
	    r_Dq1 += sgeo[idG]*r_dq24; idG+=p_Nq2;
	    r_Dq2 += sgeo[idG]*r_dq24;
	  }    
	  
	  s_Dq0[j][i] = r_Dq0;
	  s_Dq1[j][i] = r_Dq1;

	  // do this here to hide latency of smem write ?
	  for(int n=0;n<p_Nq;++n){
	    r_Aq[n] += s_D[k][n]*r_Dq2; // spread over Aq pencil
	  }
	}
      }

      barrier(localMemFence);
      
      // compute second (weak) derivatives
      for(int j=0;j<p_Nq;++j;inner1){
	for(int i=0;i<p_Nq;++i;inner0){
	  
	  dfloat Aq0 = 0.f, Aq1 = 0.f;
	  
	  for(int n=0;n<p_Nq;++n){
	    // ILP ?
	    Aq0 += s_D[n][i]*s_Dq0[j][n];
	    Aq1 += s_D[n][j]*s_Dq1[n][i];
	  }
	  
	  r_Aq[k] += Aq0 + Aq1;

	  // could do this part before barrier
	  // add (v, {dq/dn}) + (-v, tau*[q]) terms
	  // faces: (0,5) (not divergent)
	  if(k==0 || k==p_Nq-1){
	    int f = (k==0)?0:5;
	    int idG = p_Nsgeo*e*p_Nq2*p_Nfaces + p_Nsgeo*f*p_Nq2 + j*p_Nq + i; 
	    int idM = e*p_Nq2*p_Nfaces + f*p_Nq2 + j*p_Nq + i; 
	    
	    r_Aq[k] += -sgeo[idG+ p_idsJ*p_Nq2]*(dqdnP[idM]+dqdnM[idM]);
	    r_Aq[k] +=  sgeo[idG+p_idtau*p_Nq2]*r_dq05; // note dq0 is exclusive
	  }
    
	  // faces: (1,3) warp divergence
	  if(j==0 || j==p_Nq-1){
	    int f = (j==0)?1:3;
	    int idG = p_Nsgeo*e*p_Nq2*p_Nfaces + p_Nsgeo*f*p_Nq2 + k*p_Nq + i;
	    int idM = e*p_Nq2*p_Nfaces + f*p_Nq2 + k*p_Nq + i;
	    
	    // could prefetch traces
	    r_Aq[k] += -sgeo[idG+ p_idsJ*p_Nq2]*(dqdnP[idM]+dqdnM[idM]);
	    r_Aq[k] +=  sgeo[idG+p_idtau*p_Nq2]*r_dq13;
	  }

	  // faces: (2,4) (warp divergence)
	  if(i==0 || i==p_Nq-1){ 
	    int f = (i==0)?4:2;
	    int idG = p_Nsgeo*e*p_Nq2*p_Nfaces + p_Nsgeo*f*p_Nq2 + k*p_Nq + j;
	    int idM = e*p_Nq2*p_Nfaces + f*p_Nq2 + k*p_Nq + j;

	    // check signs
	    r_Aq[k] += -sgeo[idG+ p_idsJ*p_Nq2]*(dqdnP[idM]+dqdnM[idM]);
	    r_Aq[k] +=  sgeo[idG+p_idtau*p_Nq2]*r_dq24;
	  }    
	}
      }
    }
    
    // need this k-loop to be separate since r_Aq is built progressively
    for(int k=0;k<p_Nq;++k){
      for(int j=0;j<p_Nq;++j;inner1){
	for(int i=0;i<p_Nq;++i;inner0){
	  const int id = i + j*p_Nq + k*p_Nq2 + e*p_Nq3;
	  Aq[id] = r_Aq[k];
	}
      }
    }
  }
}
